{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modeling and Evaluation Regression Predict Wing Span Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a regression model to predict Wing Span for Single piston Engined airplanes.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/airplane_performance_study.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* feature_importance (3 files) \n",
        "* errors_analysis\n",
        "* Predicted vs Actual Plot\n",
        "* Residuals Distribution Plot\n",
        "* Residuals vs Fitted Plot\n",
        "* Train set\n",
        "* Test set\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* Access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to not compare apples with oranges the code below filters out all Multi Engined, jet and propjet leaving only the subset of **single Piston Engines**. It drops these columns as well as the performance modification (TP mods) column and the Hmax_(One) and ROC_(One)-columns since they only pertain to Multi Engined airplane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .query(\"Engine_Type == 0 and Multi_Engine == 0 and TP_mods == 0\")  # subset of airplanes with single Piston Enginees without any Turbo Prop performance modification TP_mods dmultiple Engines\n",
        "      .drop(labels=['Model', 'Company', 'THR', 'SHP', 'Engine_Type', 'Multi_Engine', 'TP_mods', 'Hmax_(One)', 'ROC_(One)'], axis=1)\n",
        "      )\n",
        "\n",
        "print(df.shape)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define features and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define features and target\n",
        "\n",
        "X are the features for the model (all columns in df except for target Wing_Span.\n",
        "y is the target variable which is the Wing_Span column in the df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = df.drop('Wing_Span', axis=1)  # Features (where I drop the target variable)\n",
        "y = df['Wing_Span']  # Target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train Test Set\n",
        "\n",
        "80% of df is split into the train set and 20% is left for the test set.\n",
        "The random_state=0 parameter ensures reproducibility of the split.\n",
        "\n",
        "We see that we have a suffiecent numper of data points to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # 80% to train and 20% to test.\n",
        "\n",
        "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
        "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MP Pipeline: Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipeline\n",
        "\n",
        "'scaler' applies standard scaling to the (numerical) features, ensuring that they have a mean = 0 and a standard deviation = 1.\n",
        "'regressor' initializes Random Forest regression model for predicting target variable (Wing_Span)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),  # Scale numerical features\n",
        "    ('regressor', RandomForestRegressor(random_state=0))  # Regression model using Random Forrest\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model\n",
        "\n",
        "build the training model using the training dataset.\n",
        "\n",
        "StandardScaler is applied to scale the features in X_train.\n",
        "\n",
        "RandomForestRegressor is trained on the scaled features (from previous step) and corresponding target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make predictions\n",
        "\n",
        "Scaling test features using StandardScaler fitted during training.\n",
        "\n",
        "Using the trained RandomForestRegressor to predict on scaled test data.\n",
        "\n",
        "Output: The predicted values for target (Wing_Span) stored in the predictions variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Squared Error Calculation\n",
        "\n",
        "The mean_squared_error computes the mean squared error (MSE) between the actual values (y_test) and the predicted values (predictions). The MSE provids a quantitative measure of the model's performance on the test dataset.\n",
        "\n",
        " MSE is ok but not great, typically off by almost 3 ft (square root on MSE)! However we will do a cross-validation downstreams to see if this more reliable value is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print feature importances (for RandomForestRegressor)\n",
        "\n",
        "feature_importance_df is created that in descending order (by importance) list an \"importance score\"-for each feature providing insights into which features are most influential in predicting the target variable (Wing_Span).It makes much sense that the weights are on top since the Wing Span is related to the Wing Area (not in the data set) and that Wing Area is directly related to lift that in turn is directly related to the weights (since the lift need to offset the weight in order for the airplane to fly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = pipeline.named_steps['regressor'].feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save above output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the feature names as a .pkl file\n",
        "joblib.dump(feature_names.tolist(), 'outputs/ml_pipeline/predict_analysis/feature_names.pkl')\n",
        "\n",
        "# Save the feature importance DataFrame as a .pkl file\n",
        "joblib.dump(feature_importance_df, 'outputs/ml_pipeline/predict_analysis/feature_importance_df.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicted vs actual values\n",
        "The points in the scatter plot represent the predicted Wing_Span values (on the y-axis) against the actual Wing_Span values (on the x-axis) for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, predictions, alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonal line\n",
        "plt.title('Predicted vs Actual Wing Span')\n",
        "plt.xlabel('Actual Wing Span')\n",
        "plt.ylabel('Predicted Wing Span')\n",
        "plt.xlim([y_test.min(), y_test.max()])\n",
        "plt.ylim([y_test.min(), y_test.max()])\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/predicted_vs_actual.png')  # Save the plot as a PNG file\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual plot\n",
        "\n",
        "Calculating Residuals: The residuals are calculated by subtracting the predicted values (predictions) from the actual values (y_test). Residuals indicate the difference between what the model predicted and what the actual values were.\n",
        "\n",
        "This histogram helps visualize the distribution of the residuals, which can indicate how well the model is performing. Ideally, residuals should be normally distributed around zero if the model is a good fit.\n",
        "\n",
        "The Residuals Distribution shows that the distribution of errors (if negelecting the outlier) are such that all errors are all well within +- 5 meter. (and that the model rather tend overestimate than underestimate the wingspan)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals = y_test - predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Residuals Distribution')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/residuals_distribution.png')  # Save the plot as a PNG file\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residuals vs Fitted plot\n",
        "\n",
        "Each point represents a fitted value (predicted wingspan) from the train set plotted against its corresponding residual.\n",
        "\n",
        "Ideally, the residuals should be randomly scattered around the horizontal line at zero without any discernible pattern however our pattern appear to be \"high\" which is consistent with our previous plots.\n",
        "\n",
        "If we the \"High pattern\" is real it could indicate: it may indicate issues such as non-linearity, omitted variables, or model miss-specification.\n",
        "\n",
        "We also notice an outlier at the bottom of the graf that we could hunt down and remove since it potentially could skew the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(predictions, residuals, alpha=0.6)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted')\n",
        "plt.xlabel('Fitted Values (Predicted Wing Span)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/residuals_vs_fitted.png')  # Save the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Validation with K-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation gives a more reliable estimate of model performance and shows that our model is more reliable than we thought before having made the cross-validation (MSE before minus KSA after cross-validation) in MSE with 7.26.. - 4.41.. = 2,85.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of folds\n",
        "# The selection of 10 folds is standard for more reliable estimates especially if the size of the data set allows which it does in our case.\n",
        "#\n",
        "n_folds = 10 \n",
        "\n",
        "# Initialize KFold cross-validator\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
        "\n",
        "# Perform cross-validation and calculate MSE for each fold\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Since cross_val_score returns negative MSE, we convert it to positive\n",
        "cv_scores = -cv_scores\n",
        "\n",
        "mse_k_means = cv_scores.mean()\n",
        "\n",
        "# Print out the cross-validation results\n",
        "print(f'Cross-validation Mean Squared Errors for each fold: {cv_scores}')\n",
        "print(f'Average MSE across {n_folds} folds: {mse_k_means}')\n",
        "\n",
        "# Save MSE to a text file\n",
        "with open('outputs/ml_pipeline/predict_analysis/mse_cv.txt', 'w') as f:\n",
        "    f.write(f'Cross-validation Mean Squared Errors for each fold: {cv_scores}\\n')\n",
        "    f.write(f'Average MSE across {n_folds} folds: {cv_scores.mean()}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of model prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All previous plots have appearntly given a slight overestimation of the Wingspan. This is confirmed by a \n",
        "Mean Error (ME) of approximately 5%. The Relative Error of 0.18% (based on a Wingspan range for the y_test of 29.08 feet) shows that this is error is not significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From now on we will use the result of the Cross-validation\n",
        "mse = mse_k_means\n",
        "\n",
        "# Mean Error (ME)\n",
        "mean_error = np.mean(predictions - y_test)\n",
        "print(\"Mean Error (ME):\", mean_error)\n",
        "\n",
        "# Mean Absolute Error (MAE)\n",
        "mae = np.mean(np.abs(predictions - y_test))\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "# Mean value of y_test (used for relative error calculation)\n",
        "mean_y_test = np.mean(y_test)\n",
        "\n",
        "# Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Relative Error (using RMSE)\n",
        "relative_error_rmse = rmse / mean_y_test\n",
        "print(f\"Relative Error (using RMSE): {relative_error_rmse:.4f} ({relative_error_rmse * 100:.2f}%)\")\n",
        "\n",
        "# R-Squared (R²)\n",
        "y_mean = np.mean(y_test)\n",
        "ss_total = np.sum((y_test - y_mean) ** 2)\n",
        "ss_residual = np.sum((y_test - predictions) ** 2)\n",
        "r_squared = 1 - (ss_residual / ss_total)\n",
        "print(\"R-Squared (R²):\", r_squared)\n",
        "\n",
        "# Save the results to a text file\n",
        "output_path = 'outputs/ml_pipeline/predict_analysis/error_analysis.txt'\n",
        "with open(output_path, 'w') as f:\n",
        "    f.write(f'Mean Error (ME): {mean_error:.4f} feet\\n')\n",
        "    f.write(f'Mean Absolute Error (MAE): {mae:.4f} feet\\n')\n",
        "    f.write(f'Mean Squared Error (MSE): {mse:.4f} feet\\n')\n",
        "    f.write(f'Root Mean Squared Error (RMSE): {rmse:.4f} feet\\n')\n",
        "    f.write(f'Relative Error using RMSE (RE)): {relative_error_rmse:.4f} ({relative_error_rmse * 100:.2f}%)\\n')\n",
        "    f.write(f'R-Squared (R²): {r_squared:.4f}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code for generating confusion matrix where the numerical continuous values has been binned into segments. Not that the code is cleaning/dropping NaN (due to no data points in those \"bucket\"-intervals) that, if they would not be removed, would break the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_confusion_matrix(y_test, predictions, bins, bin_labels):\n",
        "    \"\"\"\n",
        "    Creates and visualizes both a normalized and raw confusion matrix by discretizing\n",
        "    the continuous target and prediction values into bins.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_test : array-like\n",
        "        The true values of the target variable (e.g., `Wing_Span`).\n",
        "        \n",
        "    predictions : array-like\n",
        "        The predicted values of the target variable (e.g., predicted `Wing_Span`).\n",
        "        \n",
        "    bins : list\n",
        "        A list of bin edges used to discretize the continuous values.\n",
        "        \n",
        "    bin_labels : list\n",
        "        A list of labels for each bin, corresponding to the bin edges.\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "    \n",
        "    # Discretize the actual values and predictions into the bins (using the bin edges)\n",
        "    y_test_binned = pd.cut(y_test, bins=bins, labels=False, right=False)  # right=False makes bins left-inclusive\n",
        "    predictions_binned = pd.cut(predictions, bins=bins, labels=False, right=False)\n",
        "    \n",
        "    # Calculate confusion matrix using numerical indices for bins\n",
        "    cm = confusion_matrix(y_test_binned, predictions_binned)\n",
        "    \n",
        "    # Normalize the confusion matrix (by row, i.e., actual values)\n",
        "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize by row (actual values)\n",
        "    \n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Raw confusion matrix plot (without normalization)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=bin_labels, yticklabels=bin_labels)\n",
        "    plt.title('Raw Confusion Matrix')\n",
        "    plt.xlabel('Predicted Wing Span')\n",
        "    plt.ylabel('Actual Wing Span')\n",
        "\n",
        "    # Normalized confusion matrix plot (percentage format)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.heatmap(cm_percentage, annot=True, fmt='.2%', cmap='Blues', xticklabels=bin_labels, yticklabels=bin_labels)\n",
        "    plt.title('Normalized Confusion Matrix')\n",
        "    plt.xlabel('Predicted Wing Span')\n",
        "    plt.ylabel('Actual Wing Span')\n",
        "\n",
        "    # Save the confusion matrix plot (both raw and normalized)\n",
        "    plt.savefig(\"outputs/ml_pipeline/predict_analysis/confusion_matrix.png\")\n",
        "\n",
        "    # Adjust layout and show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cm, cm_percentage\n",
        "\n",
        "def clean_data(y_test, predictions):\n",
        "    \"\"\"\n",
        "    Cleans the input data by removing NaN or infinite values.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_test : array-like\n",
        "        The true values of the target variable (e.g., `Wing_Span`).\n",
        "        \n",
        "    predictions : array-like\n",
        "        The predicted values of the target variable (e.g., predicted `Wing_Span`).\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    cleaned_y_test : array-like\n",
        "        Cleaned true values of the target variable (no NaN or infinite values).\n",
        "        \n",
        "    cleaned_predictions : array-like\n",
        "        Cleaned predicted values (no NaN or infinite values).\n",
        "    \"\"\"\n",
        "    # Remove NaN or infinite values\n",
        "    mask = np.isfinite(y_test) & np.isfinite(predictions)\n",
        "    cleaned_y_test = y_test[mask]\n",
        "    cleaned_predictions = predictions[mask]\n",
        "    \n",
        "    return cleaned_y_test, cleaned_predictions\n",
        "\n",
        "# Example usage of the function:\n",
        "min_wing_span = 16\n",
        "max_wing_span = 104\n",
        "\n",
        "# Create 5 equally spaced bins between the minimum and maximum values\n",
        "bins_5 = np.linspace(min_wing_span, max_wing_span, 6)  # 6 edges = 5 bins\n",
        "print(\"5 Bins Edges:\", bins_5)\n",
        "\n",
        "# Create bin labels based on the bin edges\n",
        "bin_labels_5 = [f'{round(bins_5[i], 1)} to {round(bins_5[i+1], 1)}' for i in range(len(bins_5)-1)]\n",
        "print(\"Bin Labels:\", bin_labels_5)\n",
        "\n",
        "# Check if y_test or predictions contain NaN or Inf values\n",
        "print(\"Checking for NaN or Inf values in y_test:\")\n",
        "print(f\"y_test contains NaN: {np.any(np.isnan(y_test))}\")\n",
        "print(f\"y_test contains Inf: {np.any(np.isinf(y_test))}\")\n",
        "\n",
        "print(\"Checking for NaN or Inf values in predictions:\")\n",
        "print(f\"predictions contains NaN: {np.any(np.isnan(predictions))}\")\n",
        "print(f\"predictions contains Inf: {np.any(np.isinf(predictions))}\")\n",
        "\n",
        "# If any NaN or Inf values found, remove or replace them\n",
        "valid_mask = ~np.isnan(y_test) & ~np.isinf(y_test) & ~np.isnan(predictions) & ~np.isinf(predictions)\n",
        "y_test_clean = y_test[valid_mask]\n",
        "predictions_clean = predictions[valid_mask]\n",
        "\n",
        "# Call the function with the actual and predicted values\n",
        "confusion_matrix_result, normalized_confusion_matrix = create_confusion_matrix(y_test_clean, predictions_clean, bins_5, bin_labels_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push files to the repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuJ5tYUC06o"
      },
      "source": [
        "The following plots has already been save upstream right after they where created:\n",
        "* feature_importance (3 files) \n",
        "* Mean Squared Error\n",
        "* Predicted vs Actual Plot\n",
        "* Residuals Distribution Plot\n",
        "* Residuals vs Fitted Plot\n",
        "\n",
        "Below we will generate the following files:\n",
        "* Train set\n",
        "* Test set\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(pipeline, 'outputs/ml_pipeline/predict_analysis/wingspan_predictor_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vBpPvnaG5Mb"
      },
      "outputs": [],
      "source": [
        "file_path = f'outputs/ml_pipeline/predict_analysis'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TvoMsi3DNw1"
      },
      "source": [
        "## Train Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJHmwyqgDOr1"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv('outputs/ml_pipeline/predict_analysis/X_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB6pjmAcDOym"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.to_csv('outputs/ml_pipeline/predict_analysis/y_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbS3OnRDYtJ"
      },
      "source": [
        "## Test Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgF38n1DaPp"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test.to_csv('outputs/ml_pipeline/predict_analysis/X_test_head.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jz66iMaDacI"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test.to_csv('outputs/ml_pipeline/predict_analysis/y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Predict Tenure.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
