{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modeling and Evaluation Cluster Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/airplane_performance_study.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* Access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the parent of the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fix for unsolvable api version conflict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install --no-cache-dir numpy>=1.20.3 scikit-learn==1.2.0 yellowbrick==1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: numpy\n",
            "Version: 1.19.5\n",
            "Summary: NumPy is the fundamental package for array computing with Python.\n",
            "Home-page: https://www.numpy.org\n",
            "Author: Travis E. Oliphant et al.\n",
            "Author-email: \n",
            "License: BSD\n",
            "Location: /workspace/.pip-modules/lib/python3.8/site-packages\n",
            "Requires: \n",
            "Required-by: altair, contourpy, feature-engine, ImageHash, imbalanced-learn, matplotlib, pandas, patsy, phik, pyarrow, pydeck, PyWavelets, scikit-learn, scipy, seaborn, statsmodels, streamlit, visions, wordcloud, xgboost, ydata-profiling, yellowbrick\n"
          ]
        }
      ],
      "source": [
        "#!pip show numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl.metadata (9.8 kB)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: yellowbrick==1.3 in /workspace/.pip-modules/lib/python3.8/site-packages (1.3)\n",
            "Requirement already satisfied: ydata-profiling==4.4.0 in /workspace/.pip-modules/lib/python3.8/site-packages (4.4.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.9.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /workspace/.pip-modules/lib/python3.8/site-packages (from scikit-learn==0.24.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from scikit-learn==0.24.2) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from yellowbrick==1.3) (3.6.0)\n",
            "INFO: pip is looking at multiple versions of yellowbrick to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install numpy==1.23.5, scikit-learn==0.24.2 and yellowbrick==1.3 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested numpy==1.23.5\n",
            "    scikit-learn 0.24.2 depends on numpy>=1.13.3\n",
            "    yellowbrick 1.3 depends on numpy<1.20 and >=1.16.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!pip install --no-cache-dir scikit-learn==0.24.2 numpy==1.23.5 yellowbrick==1.3 ydata-profiling==4.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip show numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:23\u001b[0m\n\u001b[1;32m     19\u001b[0m     np_percentile_argname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nlv \u001b[38;5;241m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis version of pandas is incompatible with numpy < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to use this pandas version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     30\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_np_version\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_numpy_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m ]\n",
            "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler  # Feat Scaling\n",
        "from sklearn.decomposition import PCA  # PCA\n",
        "from sklearn.cluster import KMeans  # ML algorithm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel  # Feat Selection\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # ML algorithm\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are dropping:\n",
        "* Model, Company because these are only meta data/identifiers\n",
        "* 'Multi_Engine' and 'Engine_Type' becasue we want to evaluate against these\n",
        "* THR and SHP both because they are difficult to interpret\n",
        "* Hmax_(One) and ROC_(One) becasue since they ONLY apply to the Airplanes with a \"True\" under the \"Multi Engine\"-feature it would therefore skew the result. A run that included Hmax_(One) and ROC_(One) indeed show Hmax_(One) on top as the most important feature however ROC_(One) was even listed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .drop(['Model', 'Company', 'Multi_Engine', 'Engine_Type', 'THR', 'SHP', 'Hmax_(One)', 'ROC_(One)'], axis=1)\n",
        "      )\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "##  ML Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert boolean columns to strings\n",
        "df['TP_mods'] = df['TP_mods'].replace({True: 'Yes', False: 'No'})\n",
        "\n",
        "# Check the updated data types\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use K-means since this model is ideal for numerical continuous data and the majority of our data is numerical continuous. However it is mixed (both categorical and numeric continuous) which could warrant another method like K-modes or K-prototypes that are better suited for mixed data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Threshold is set to 0.5 which is a very low (values are typically 0.85 - 0.9) which is first of all since our values are relatively normally distributed and we shall see that even this low threshold retains only two features! We have experimented with many different threshold values without seeing any significantly difference on n_components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Constructs a machine learning pipeline for clustering.\n",
        "\n",
        "    The pipeline includes steps for feature encoding, correlation-based feature selection,\n",
        "    feature scaling, dimensionality reduction, and KMeans clustering.\n",
        "\n",
        "    Steps:\n",
        "    1. Encodes categorical variables using OrdinalEncoder.\n",
        "    2. Selects features with SmartCorrelatedSelection to reduce multicollinearity.\n",
        "    3. Scales features with StandardScaler.\n",
        "    4. Applies PCA for dimensionality reduction to 50 components.\n",
        "    5. Performs KMeans clustering to identify 50 clusters.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object for clustering tasks.\n",
        "\n",
        "    Categorical Variables:\n",
        "        - TP_mods\n",
        "        - Engine_Type\n",
        "\n",
        "    KMeans Parameters:\n",
        "        - n_clusters: 50\n",
        "        - random_state: 0\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We only recieve 2 components. Since the components is 0% and 100% the elbow method (select a number of components around a variance of 80%.) is rendered useless. Other dimensionality reduction techniques like t-SNE or UMAP could be considered.\n",
        "\n",
        "The \"SmartCorrelatedSelection\" reduces the number of rows (dimensionality reduction) depending on what threshold value we set! https://feature-engine.trainindata.com/en/latest/user_guide/selection/SmartCorrelatedSelection.html\n",
        "\n",
        "We are not concerned with the warnings below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "Apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_pca[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 2  # Adapted this to two since this value cannot be higher than\n",
        "# the number of retained columns in the df_pca which was only three with a\n",
        "# threshold value as low as 0,6\n",
        "\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes the explained variance of PCA components.\n",
        "\n",
        "    This function fits PCA to the provided DataFrame, calculates the explained variance ratio\n",
        "    for each principal component, and plots both the explained and accumulated variance.\n",
        "\n",
        "    Parameters:\n",
        "        df_pca (DataFrame): A pandas DataFrame containing standardized data for PCA.\n",
        "        n_components (int): The number of principal components to retain (must be â‰¤ number of features).\n",
        "\n",
        "    Outputs:\n",
        "        Prints the total percentage of variance explained by the specified components and displays a line plot.\n",
        "\n",
        "    Notes:\n",
        "        Ensure `df_pca` is preprocessed for PCA suitability.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                      for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_components_analysis(df_pca=df_pca,n_components=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Plot with Accumulated CVariance Ratio incldue only 2 components (n_components=2) and we select to retain both thesse since our main purpos with this \n",
        "analysis is interpretability and we want to retain as much information as possible for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Creates a clustering pipeline with feature encoding, selection, scaling, PCA, and KMeans.\n",
        "\n",
        "    This function constructs a scikit-learn Pipeline that:\n",
        "    1. Encodes categorical variables using OrdinalEncoder.\n",
        "    2. Applies Smart Correlated Selection to filter features based on correlation.\n",
        "    3. Scales features with StandardScaler.\n",
        "    4. Performs PCA to reduce dimensionality to 2 components.\n",
        "    5. Clusters the data into 50 groups using KMeans.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object ready for fitting and transforming data.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # we update n_components to 2\n",
        "        (\"PCA\", PCA(n_components=2, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "outputs": [],
      "source": [
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive, it will plot until 10\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6 is not inclusive, it will stop at 5\n",
        "n_cluster_start, n_cluster_stop = 2, 6\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Elbow Method gives a cluster size of two or three. The Silhouette shadow plots appear to clearly point towards 2 clusters so this will be our choice. With two clusters both reaches past the mean line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Constructs a clustering pipeline for feature processing and KMeans clustering.\n",
        "\n",
        "    This function creates a scikit-learn Pipeline that:\n",
        "    1. Encodes categorical variables with OrdinalEncoder.\n",
        "    2. Filters features using Smart Correlated Selection based on correlation.\n",
        "    3. Scales features with StandardScaler.\n",
        "    4. Reduces dimensionality to 2 components using PCA.\n",
        "    5. Clusters data into 2 groups using KMeans.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object configured for clustering.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=2, random_state=0)),\n",
        "\n",
        "        # we update n_clusters to 2\n",
        "        (\"model\", KMeans(n_clusters=2, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "Quick recap of our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0iMkjJHXSI"
      },
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "We add a column \"Clusters\" (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_analysis.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],  # Original code is: y=df_analysis[:, 1] but that gives an error since it tries to acces the second column of a one column df\n",
        "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 1], y=pipeline_cluster['model'].cluster_centers_[:, 1],  # Same here! Original code: cluster_centers_[:, 0]\n",
        "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fact that the centroid for the smaller cluster (blue) is far away (lateraly) from the actual datapoint (but close longitudinally) might suggest low intrinsic dimensionality that the n_components should have been choosen to 1 instead of 2. Other reasons could be: Poor Cluster Quality, Outliers, High Variability or Feature Scaling Issues.\n",
        "\n",
        "The alignment of clusters along lines indicates that there are strong linear relationships in the data and therefore perhaps the cluster patterns could be captured with fewer components. The linear relationships probably has the background that the performance features follow similar patterns (what is good for one feature is also good for another feature).The fact that the clusters are clearly spaced from each other, like the smallest (green) cluster suggest that this is a categorical feature that \"lifts\" all the other features an equal part. Such a candidate would me the TP mods that we have seen stick out before. This is also one of only two categorical values since we have dropped Multi Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWgb0kPOWtMa"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "## Fit a classifier, where the target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6sGUn0XyDm"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "Create classifier pipeline steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "    \"\"\"\n",
        "    Creates a classification pipeline with feature selection and scaling.\n",
        "\n",
        "    This function constructs a scikit-learn Pipeline that performs the following steps:\n",
        "    1. Encodes categorical variables using OrdinalEncoder.\n",
        "    2. Selects features based on correlation with the target variable using Smart Correlation Selection.\n",
        "    3. Scales features with StandardScaler.\n",
        "    4. Applies SelectFromModel with a Gradient Boosting Classifier to select important features.\n",
        "    5. Trains a Gradient Boosting Classifier on the selected features.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object for classification tasks.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"feat_selection\", SelectFromModel(\n",
        "            GradientBoostingClassifier(random_state=0))),\n",
        "\n",
        "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "## Assess the most important Features that define a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most important feature being TP mods is rather disappointing since this is a trivial result from an Aircraft Design point of view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after data cleaning and feature engineering, the feature space changes\n",
        "\n",
        "data_cleaning_feat_eng_steps = 2\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
        ")].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# reassign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgul0EF9nx_E"
      },
      "source": [
        "We will store the best_features to use at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzyMkwHznyG8"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "Load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "outputs": [],
      "source": [
        "def DescriptionAllClusters(df, decimal_points=3):\n",
        "    \"\"\"\n",
        "    Generate descriptive statistics for each cluster in the provided DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing cluster data, including a 'Clusters' column.\n",
        "    decimal_points (int): Number of decimal points to format numeric summaries.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame with descriptive statistics for each cluster.\n",
        "    \"\"\"\n",
        "    DescriptionAllClusters = pd.DataFrame(\n",
        "        columns=df.drop(['Clusters'], axis=1).columns)\n",
        "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
        "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "        EDA_ClusterSubset = df.query(\n",
        "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
        "        ClusterDescription = Clusters_IndividualDescription(\n",
        "            EDA_ClusterSubset, cluster, decimal_points)\n",
        "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
        "            ClusterDescription)\n",
        "\n",
        "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
        "    return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
        "\n",
        "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "    \"\"\"\n",
        "    Generate descriptive statistics for a specific cluster's features.\n",
        "\n",
        "    Parameters:\n",
        "    EDA_Cluster (pd.DataFrame): DataFrame containing features of a specific cluster.\n",
        "    cluster (any): Identifier for the cluster being analyzed.\n",
        "    decimal_points (int): Number of decimal points for rounding numeric statistics.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame summarizing the statistics of the cluster's features.\n",
        "    \"\"\"\n",
        "    # for a given cluster, iterate over all columns\n",
        "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
        "    # That will show the range for the most common values for the numerical variable\n",
        "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
        "    # That will show the most common levels for the category\n",
        "\n",
        "    for col in EDA_Cluster.columns:\n",
        "\n",
        "        try:  # eventually a given cluster will have only missing data for a given variable\n",
        "\n",
        "            if EDA_Cluster[col].dtypes == 'object':\n",
        "\n",
        "                top_frequencies = EDA_Cluster.dropna(\n",
        "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "                Description = ''\n",
        "\n",
        "                for x in range(len(top_frequencies)):\n",
        "                    freq = top_frequencies.iloc[x]\n",
        "                    category = top_frequencies.index[x][0]\n",
        "                    CategoryPercentage = int(round(freq*100, 0))\n",
        "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
        "                    Description = Description + statement\n",
        "\n",
        "                ClustersDescription.at[0, col] = Description[:-2]\n",
        "\n",
        "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
        "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
        "                Description = f\"{Q1} -- {Q3}\"\n",
        "                ClustersDescription.at[0, col] = Description\n",
        "\n",
        "        except Exception as e:\n",
        "            ClustersDescription.at[0, col] = 'Not available'\n",
        "            print(\n",
        "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "\n",
        "    ClustersDescription['Cluster'] = str(cluster)\n",
        "\n",
        "    return ClustersDescription\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHo7wmH68AYc"
      },
      "source": [
        "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NN23X2dT8AeA"
      },
      "outputs": [],
      "source": [
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "\n",
        "    Visualizes the distribution of clusters across a specified variable.\n",
        "\n",
        "    This function generates two plots using Plotly:\n",
        "    1. A bar plot showing the count of each target level within each cluster.\n",
        "    2. A line plot depicting the relative percentage of each target level in each cluster.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): A pandas DataFrame containing cluster predictions and the target variable.\n",
        "        target (str): The name of the variable to analyze against the clusters.\n",
        "\n",
        "    Returns:\n",
        "        None: Displays plots directly in the Jupyter notebook.\n",
        "\n",
        "    Notes:\n",
        "        The DataFrame must include a 'Clusters' column for cluster assignments.\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
        "                 color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.show(renderer='jupyterlab')\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .groupby(level=0)\n",
        "                   .apply(lambda x:  100*x / x.sum())\n",
        "                   .reset_index()\n",
        "                   .sort_values(by=['Clusters'])\n",
        "                   )\n",
        "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
        "                  color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show(renderer='jupyterlab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJRrFc7wzu"
      },
      "source": [
        "We want also to analyse distribution of Multi Engine and Engine Types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSRSNqiF4mnm"
      },
      "outputs": [],
      "source": [
        "df_multi_engine = pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\").filter(['Multi_Engine'])\n",
        "df_multi_engine['Multi_Engine'] = df_multi_engine['Multi_Engine'].astype('object')\n",
        "df_multi_engine.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also testing for Engine Type albeit not the aim for the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engine_type = pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\").filter(['Engine_Type'])\n",
        "df_engine_type['Engine_Type'] = df_engine_type['Engine_Type'].astype('object')\n",
        "df_engine_type.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD0Y3NdJOhm"
      },
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Multi Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile_multi_engine = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_multi_engine], axis=1), decimal_points=0)\n",
        "clusters_profile_multi_engine.to_csv(f\"outputs/ml_pipeline/cluster_analysis/v1/clusters_profile_multi_engine.csv\")\n",
        "clusters_profile_multi_engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and for Engine Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile_engine_type = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_engine_type], axis=1), decimal_points=0)\n",
        "clusters_profile_engine_type.to_csv(f\"outputs/ml_pipeline/cluster_analysis/v1/clusters_profile_engine_type.csv\")\n",
        "clusters_profile_engine_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above Cluster Profile is very important since we from this plot can draw conclusions of the patterns.\n",
        "\n",
        "The description of the clusters tells us not surprisingly that the modification to the Engine Performance (TP_mods) was perfectly clustered and that Airplanes without TP mods are pretty much fifty-fifty when it comes to Multi Engine. Furthermore it shows that the TP mods all most only appear on Piston Engines.\n",
        "\n",
        "The most important conclusion from this clustering exercise is that TP mods appear to be:\n",
        "* Exclusively made on piston powered airplanes!\n",
        "* more common on Multi Engine Airplanes where 2/3 of the TP mods being done on Multi Engines.\n",
        "\n",
        "Since only one feature has been retained together with that the value of this clustering exercise has proved to be disappointing makes us not continue any furter with a re-fitting of the pipeline.\n",
        "\n",
        "The reason that this clustering did not yield much valuable insight is most likely becaus the the data set was to homogenous in terms of airplane types and that the airplanes in the data set was quite evenly and continuously distributed in terms of features such as size for example. If we would instead would have dropped the mid sized airplanes or introduced another type of airplanes such as fighter jets or drones we would most likely have experienced a more clear clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SS6CCCb74lH"
      },
      "source": [
        "### Clusters distribution across Multi Engine and Engine Type distributions & Relative Percentage of Multi Engine and Engine Type in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_multi_engine=  df_multi_engine.copy()\n",
        "df_cluster_vs_multi_engine['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_multi_engine, target='Multi_Engine')\n",
        "plt.savefig(f\"outputs/ml_pipeline/cluster_analysis/v1/cluster_vs_multi_engine.png\", bbox_inches='tight', dpi=150)  # Save plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_engine_type=  df_engine_type.copy()\n",
        "df_cluster_vs_engine_type['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_engine_type, target='Engine_Type')\n",
        "plt.savefig(f\"outputs/ml_pipeline/cluster_analysis/v1/cluster_vs_engine_type.png\", bbox_inches='tight', dpi=150) # Save plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpjktKd1n6"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i9X1oOORAQc"
      },
      "source": [
        "\n",
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySBIrV1Q4cY"
      },
      "outputs": [],
      "source": [
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9-0fisd5cl"
      },
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfv9k5xMd7fv"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsphnIR84hJ4"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ORnkwG6d74O"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcwHaVwd9Ff"
      },
      "outputs": [],
      "source": [
        "df_cluster_study=df\n",
        "print(df.shape)\n",
        "df_cluster_study.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "M26MiJ9Y485Q"
      },
      "outputs": [],
      "source": [
        "df_cluster_study.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_lcQVXaV0p"
      },
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9datNfsLCVV"
      },
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYeoH7fjaV8J"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr0cVVQsaZqk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RObeac1HQq5a"
      },
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_analysis)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the profile results we can label the two clusters in the following fashion:\n",
        "* **Cluster 0** represent airplanes that does not have TP mods and is made up by all Engine Types (piston, propjet and jet).\n",
        "* **Cluster 1** represent airplanes that has TP mods and is almost exclusively made up by Piston powered Airplanes\n",
        "(Airplane Type: piston) indicating that the TP mods is a feature applicable only on Piston Powered Engines. Furthermore Multi Engine Airplanes represented 2/3 of all TP mods."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
